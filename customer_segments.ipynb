{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Status: In Progress (Updated: 3/14/2016)\n",
    "\n",
    "This project will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries: NumPy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"wholesale-customers.csv\")\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "\n",
    "print \"\\nStandard Deviation:\"\n",
    "print data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions.\n",
    "\n",
    "-  Principal Component Analysis (PCA) is utilized to identify patterns in dataset based on the correlation between the n-features. PCA attempts to find the directions of maximum variance in a high-dimensional data (d=6) and the projects it onto a new subspace (k<=d). We would expect the first several PCs to be composed of original attributes that are in some way correlated with each other. These principal components could represent the customers that purchase certain items or a combination of the items. Additionally, before running any PCA, plotting the histograms (shown below) seems to indicate a correlation in the number of orders among `Fresh`, `Milk`, and `Grocery`. Intuitively, the histograms show an exponential decline in the number of orders for the respected products, hence this could represent a cluster of the larger companies. In contrast to the remaining products, the histogram virtually drops off after the first two bins (lower number of orders) and perhaps indicate the smaller company purchases from the wholesale grocery distributor. In addition to `Fresh` and `Milk` having the highest standard deviations of 12,647 and 9,503 respectively, I would expect these two items to have a high significance (magnitude) in the eigenvector.\n",
    "\n",
    "\n",
    "- Independent Component Analysis (ICA) is a statistical technique for identifying the underlying hidden factors for a given multidimensional dataset. In short, ICA produces dimensions of variation where the dimensions (features) are as statistically independent from one another. The data variables are assumed to be linear mixtures of some unknown \"latent\" variables and the mixing system is also unknown. The assumption is that the latent variables are both nongaussian and mutually independent; these variables are called the independent components. In the context of this project we are attempting to identify the different buyer types and cluster them together. Being statistically independent, the different type of buyer typers could mean the size of the purchaser (small/larger) or different consumption patterns. For example, there are buyers who mostly purchased Milk and Grocery, and less Detergents/Paper or Delicatessen. Likely this could represent the type of purchasers for a small market or convenience store. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='hist',bins = 30, subplots=True, layout=(3,2), legend=True, figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?\n",
    "\n",
    "- The first PC accounts for the highest portion of the total variance or \"spread\" of the data. The subsequent PC's account for the remaining variability, and usually the first couple of PC's account for roughly 90-95% of the total variance. The below plot shows the variance vs the number of principal components (k=5). Based on this plot, the number of dimensions would be reduced to 2, which explains roughly 86% of the total variance. In this plot, the ['Explained Variance Ratio`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \n",
    "is a fraction of the total variance explained by each of the selected components. In regards to how quickly does the variance drop off, we can compute the slope of the number of PC components (x) vs the explained variance (y). The slope between PC 1-2 is -0.054 and the slope between 2-3 is -0.335. Furthermore, the slope between PC 3-4 is -0.026 and at this point, the slope begins to level out. Therefore between PC 2-3 you observe the steepest slope as the explained variance ratio begins to quickly decline. The bar plot below is an alternative visualization to observe the change in variance per PC. \n",
    " \n",
    " \n",
    "- As a note, for PCA to be implemented correctly, the dataset needs to be standardized (mean equal 0 and standard deviation =1. You need to address the event where features are on different scales or different magnitudes of range, and thus causing the results of PCA to be biased in the direction of the features with a larger range. One approach to scaling the data is using [`Scale`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html); centers the mean and component wise scale to unit variance (1). However, it may not always be the case in which the data should be standardized. In some instances you may want to preserve the variance of each dimension as opposed to scaling to unit length, which could remove the potential dependence between features since each feature is scaled independently. The features were not scaled to stdev = 1 for this dataset as the attributes have the same units monetary units (m.u.) and in essence, we are trying to capture the variance in spending among the different types of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3)** What do the dimensions seem to represent? How can you use this information? \n",
    "\n",
    "- In the original dimensional space the dimensions of the axes are in terms of the amount of monetary units (m.u.) the food distributor sales. The axes are then transposed into the new dimensional space and is now represented by the principal components. First, we must begin by constructing the covariance matrix (S); covariance is the measures how much the features vary from the mean with respect to each other. For this dataset, there are 440 elements (n=440) and 6 measurements (m=6), so the covariance matrix would be a 440x6 matrix; with 6 eigenvalues. The total variance is the Trace of the S, which is just the summation of the main diagonal. Next the covariance matrix is decomposed into its eigenvectors and eigenvalues; the eigenvectors of S represents the principal components (directions of maximum variance) and the eigenvalues (scalar) correspond to the magnitude of the eigenvectors. The eigenvalues are sorted in descending order (largest first) and dividing each eigenvalue by the total variance yields the fraction of variance explained. We are only interested in the top eigenvectors based on the values of their corresponding eigenvalues. The eigenvector corresponding to the highest eigenvalue will be the first PC. The eigenvector (v1) points in the direction of maximum variance (most information gained) of the dataset. The second eigenvector (v2) will point in direction orthogonal (perpendicular in 2D) to v1. One of the main advantages of using PCA is if you have a dataset with high dimensions you can reduce the number dimensions without losing much of the information. Using feature extraction, the data is projected onto a new feature space and thus reducing the number dimensions through data compression; the goal is to maintain the most relevant information. Being that the objective is to reduce the dimensionality of the data by compressing it onto a new feature subspace, we chose only the subset of the eigenvectors (principal components) that contains most of the information (variance). \n",
    "\n",
    "\n",
    "- The dimensions of the first PC (explains the most variability in the data) and has coefficients of `Fresh = -0.976`, `Frozen = -0.152`,`Milk = -0.121`, etc. We can clearly see that fresh items have the largest magnitude, followed by frozen products, and then milk. The descriptive statistics shows that on average, fresh items are purchased 55% more than the other 5 products. The annual spending on fresh items has the largest standard deviation (12,647) which suggest there is a lot of variance between the customers on how much they spend on fresh items. As the first PC identifies the item as the most important factor among the features, it would be a good idea to increase marketing towards the customer base on fresh products as it seems to be a cluster of those who buy large quantities  of fresh products while there are customers who make smaller purchase orders. \n",
    "\n",
    "\n",
    "- Second Principal Component gives the following magnitudes for the eigenvector v2: `Fresh = -0.110614` `Milk =0.515802`, `Grocery =0.764606`, `Frozen = -0.018723` etc. Looking at the magnitude of the eigenvector, a correlation exist among `Grocery` and `Milk`; customers who buy more 'grocery' items also tend to buy more 'milk' items as well. Therefore, it would be advised to market grocery and milk products together given their correlation in order to increase sales. Bundling these two items when shipping to the customer will also reduce cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA with the same number of dimensions as variables in the dataset\n",
    "columns = data.columns\n",
    "mean = data.mean()\n",
    "df_m = pd.DataFrame(mean)\n",
    "df_mean = df_m.transpose()\n",
    "data_center = pd.DataFrame(data[columns].values - df_mean[columns].values, columns=columns)\n",
    "#print data_center\n",
    "\n",
    "def doPCA():\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(data_center)\n",
    "    return pca\n",
    "\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "pca = doPCA()\n",
    "columns = data_center.columns\n",
    "df_ica = pd.DataFrame(pca.components_, columns = columns, index=['1', '2', '3', '4','5'])\n",
    "df_ica.index.names = ['PC']\n",
    "print \"Principal Component Analysis:\"\n",
    "print df_ica\n",
    "\n",
    "ex_var = pca.explained_variance_ratio_\n",
    "df_var = pd.Series(ex_var,index=['1','2','3','4','5'])\n",
    "df_var.sort(ascending=False)\n",
    "\n",
    "\n",
    "df_var.index.names = ['PC']\n",
    "print \"\\nExplained Variance of Each Component:\"\n",
    "print df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8,5))\n",
    "x = np.arange(1,6)\n",
    "y = np.cumsum(ex_var)\n",
    "pl.plot(x,y,marker =\"o\", mfc='#780000', color = '#CC0000')\n",
    "pl.xlabel(\"No. of PC Components\", fontsize = 14)\n",
    "pl.ylabel(\"Cumulative Explained Variance Ratio\", fontsize =14)\n",
    "pl.title(\"No. of PC Components vs Explained Variance Ratio\", fontsize = 16)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "pl.bar(range(1,6), ex_var, alpha = .8, align='center',\n",
    "       label = 'Individual Explained Variance', color = 'blue')\n",
    "pl.ylabel('Explained Variance Ratio', fontsize = 14)\n",
    "pl.xlabel('Princial Components', fontsize = 14)\n",
    "pl.title('Explained Variance', fontsize = 16)\n",
    "pl.show\n",
    "\n",
    "#Slope:\n",
    "X1, Y1 = 1, 0.459614\n",
    "X2, Y2 = 2, 0.405172\n",
    "X3, Y3 = 3, 0.070030\n",
    "X4, Y4 = 4, 0.044023\n",
    "slope1 = (Y2-Y1)/(X2-X1)\n",
    "slope2 = (Y3-Y2)/(X3-X2)\n",
    "slope3= (Y4-Y3)/(X4-X3)\n",
    "print \"Slope1:\", slope1\n",
    "print \"Slope2:\", slope2\n",
    "print \"Slope3:\", slope3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def biplot(df):\n",
    "    # Fit on 2 components\n",
    "    pca = PCA(n_components=2, whiten=True).fit(df)\n",
    "    \n",
    "    # Plot transformed/projected data\n",
    "    ax = pd.DataFrame(\n",
    "        pca.transform(df),\n",
    "        columns=['PC1', 'PC2']\n",
    "    ).plot(kind='scatter', x='PC1', y='PC2', figsize=(8, 6), color = 'g', s=5)\n",
    "    # Plot arrows and labels\n",
    "    for i, (pc1, pc2) in enumerate(zip(pca.components_[0], pca.components_[1])):\n",
    "        ax.arrow(0, 0, pc1, pc2, width=0.001, fc='r', ec='r')\n",
    "        ax.annotate(df.columns[i], (pc1, pc2), size=12)\n",
    "    return ax\n",
    "\n",
    "ax = biplot(data)\n",
    "pl.title(\"Biplot\", fontsize = 20)\n",
    "pl.xlabel(\"Principal Component: 1\", fontsize = 14)\n",
    "pl.ylabel(\"Principal Component: 2\", fontsize = 14)\n",
    "ax.set_xlim([-1.5, .5])\n",
    "ax.set_ylim([-1.0, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?\n",
    "\n",
    "- ICA will provide six new vectors with each vector being an independent component. With ICA, we can attempt to identify the different types of customers so that we can understand more of their consumption patterns.\n",
    "\n",
    "- ICA1: Mostly includes frozen items (highest magnitude); customers tend to buys more grocery/frozen items and less detergents_paper/delicatessen/fresh/milk products.\n",
    "- ICA2: Customer who buys more fresh products buys less milk/detergents_paper/delicatessen\n",
    "- ICA3: Most significant is orders for delicatessens while decrease in the remaining products\n",
    "- ICA4: Grocery has the highest magnitude (0.11); increase in grocery purchases is inversely related to detergents (decrease).\n",
    "- ICA5: Customer purchases mostly milk/grocery/ will buy less slightly less fresh items and detergents (perhaps small market store)\n",
    "- ICA6: Primarly as purchase orders for grocery increases, milk/fresh decrease  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adjust the data to have center at the origin:\n",
    "columns = data.columns\n",
    "scale = preprocessing.scale(data)\n",
    "df_scale = pd.DataFrame(scale, columns=columns)\n",
    "#print data_scaled.mean()\n",
    "#print data_scaled.std()\n",
    "\n",
    "def doICA():\n",
    "    ica = FastICA(n_components = 6, random_state=42)\n",
    "    ica.fit(df_scale)\n",
    "    return ica\n",
    "\n",
    "# Print the independent components\n",
    "ica = doICA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ica = pd.DataFrame(ica.components_,columns=columns, index=['1','2','3','4','5','6'])\n",
    "df_ica.index.names = ['IC']\n",
    "print \"\\nUnmixing Matrix:\"\n",
    "df_ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(20,20))\n",
    "pl.figure(figsize = (11,5))\n",
    "pl.title('Independent Component Analysis', fontsize=18)\n",
    "sns.heatmap(df_ica, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then sample elements from the clusters to understand their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Reduce to Two dimensions using PCA to capture variation\n",
    "columns_pca = ['PC1','PC2']\n",
    "reduced_data = PCA(n_components = 2).fit_transform(df_scale)\n",
    "df_rd = pd.DataFrame(reduced_data, columns=columns_pca)\n",
    "df_rd[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Implement clustering algorithm and fit it to the reduced data for visualization\n",
    "def cluster(clusterer):\n",
    "    clusterer.fit(reduced_data)\n",
    "    clusters = clusterer\n",
    "    print clusters\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_clusters():\n",
    "    distortions = []\n",
    "    for i in range(1,11):\n",
    "        km = KMeans(n_clusters = i,\n",
    "                    init='k-means++',\n",
    "                    n_init=10,\n",
    "                    max_iter=300,\n",
    "                    random_state=0)\n",
    "        km.fit(data)\n",
    "        distortions.append(km.inertia_)\n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.plot(range(1,11),distortions,marker='o', color='g')\n",
    "    pl.title('Optimal Number of Clusers', fontsize=18)\n",
    "    pl.xlabel('Number of Clusters', fontsize=14)\n",
    "    pl.ylabel('Distortion', fontsize=14)\n",
    "    pl.show\n",
    "    \n",
    "number_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary \n",
    " ###### Building Mesh Grid to Populate a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def boundary(clusters):\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    hx = (x_max-x_min)/1000.\n",
    "    hy = (y_max-y_min)/1000.\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    return Z,xx,yy,x_min,x_max,y_min,y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Find the centroids for KMeans or the cluster means for GMM \n",
    "def cluster_means(clusters, func_name):\n",
    "    centroids = getattr(clusters, func_name)\n",
    "    print centroids\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "def colorplot (clusters, Z,xx,yy,x_min,x_max,y_min,y_max,centroids):\n",
    "    pl.figure(figsize=(12,10))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure(1)\n",
    "    pl.clf()\n",
    "    pl.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=pl.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    pl.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "    pl.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    pl.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "              'Centroids are marked with white cross', fontsize = 18)\n",
    "    pl.xlim(x_min, x_max)\n",
    "    pl.ylim(y_min, y_max)\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusterplot(clusterer,func_name):\n",
    "    clusters = cluster(clusterer)\n",
    "    Z,xx,yy,x_min,x_max,y_min,y_max = boundary(clusters)\n",
    "    centroids = cluster_means(clusters,func_name)\n",
    "    colorplot(clusters,Z,xx,yy,x_min,x_max,y_min,y_max,centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusterplot(KMeans(n_clusters=4),'cluster_centers_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
