{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Status: In Progress (Updated: 3/14/2016)\n",
    "\n",
    "This project will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries: NumPy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_csv(\"wholesale-customers.csv\")\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "\n",
    "print \"\\nStandard Deviation:\"\n",
    "print data.std()\n",
    "\n",
    "X=data.ix[:,0:6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions.\n",
    "\n",
    "-  Principal Component Analysis (PCA) is utilized to identify patterns in dataset based on the correlation between the n-features. PCA attempts to find the directions of maximum variance in a high-dimensional data (d=6) and the projects it onto a new subspace (k<=d). We would expect the first several PCs to be composed of original attributes that are in some way correlated with each other. These principal components could represent the customers that purchase certain items or a combination of the items. Additionally, before running any PCA, plotting the histograms (shown below) seems to indicate a correlation in the number of orders among `Fresh`, `Milk`, and `Grocery`. Intuitively, the histograms show an exponential decline in the number of orders for the respected products, hence this could represent a cluster of the larger companies. In contrast to the remaining products, the histogram virtually drops off after the first two bins (lower number of orders) and perhaps indicate the smaller company purchases from the wholesale grocery distributor. In addition to `Fresh` and `Milk` having the highest standard deviations of 12,647 and 9,503 respectively, I would expect these two items to have a high significance (magnitude) in the eigenvector.\n",
    "\n",
    "\n",
    "- Independent Component Analysis (ICA) is a statistical technique for identifying the underlying hidden factors for a given multidimensional dataset. In short, ICA produces dimensions of variation where the dimensions (features) are as statistically independent from one another. The data variables are assumed to be linear mixtures of some unknown \"latent\" variables and the mixing system is also unknown. The assumption is that the latent variables are both nongaussian and mutually independent; these variables are called the independent components. In the context of this project we are attempting to identify the different buyer types and cluster them together. Being statistically independent, the different types of buyers could mean the size of the purchaser (small/larger) or different consumption patterns. For example, there are buyers who mostly purchased Milk and Grocery, and less Detergents/Paper or Delicatessen. Likely this could represent the type of purchasers for a small market or convenience store. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='hist',alpha=0.5,bins = 30, subplots=True, layout=(3,2), legend=True, figsize=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_matrix = data.cov()\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Eigenvalue and Eigenvector of covariance matrix\n",
    "columns=data.columns\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "sum_ev = sum(eig_vals)\n",
    "#Explained Variance Ratio\n",
    "print eig_vals/sum_ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?\n",
    "\n",
    "- The first PC accounts for the highest portion of the total variance or \"spread\" of the data. The subsequent PC's account for the remaining variability, and usually the first couple of PC's account for roughly 90-95% of the total variance. The below plot shows the variance vs the number of principal components (k=5). Based on this plot, the number of dimensions would be reduced to 2, which explains roughly 86% of the total variance. In this plot, the ['Explained Variance Ratio`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \n",
    "is a fraction of the total variance explained by each of the selected components. In regards to how quickly does the variance drop off, we can compute the slope of the number of PC components (x) vs the explained variance (y). The slope between PC 1-2 is -0.054 and the slope between 2-3 is -0.335. Furthermore, the slope between PC 3-4 is -0.026 and at this point, the slope begins to level out. Therefore between PC 2-3 you observe the steepest slope as the explained variance ratio begins to quickly decline. The bar plot below is an alternative visualization to observe the change in variance per PC. \n",
    " \n",
    " \n",
    "- As a note, for PCA to be implemented correctly, the dataset needs to be standardized (mean equal 0 and standard deviation =1. You need to address the event where features are on different scales or different magnitudes of range, and thus causing the results of PCA to be biased in the direction of the features with a larger range. One approach to scaling the data is using [`Scale`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html); centers the mean and component wise scale to unit variance (1). However, it may not always be the case in which the data should be standardized. In some instances you may want to preserve the variance of each dimension as opposed to scaling to unit length, which could remove the potential dependence between features since each feature is scaled independently. The features were not scaled to stdev = 1 for this dataset as the attributes have the same units monetary units (m.u.) and in essence, we are trying to capture the variance in spending among the different types of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3)** What do the dimensions seem to represent? How can you use this information? \n",
    "\n",
    "- In the original dimensional space the dimensions of the axes are in terms of the amount of monetary units (m.u.) the food distributor sales. The axes are then transposed into the new dimensional space and is now represented by the principal components. First, we must begin by constructing the covariance matrix (S); covariance is the measures how much the features vary from the mean with respect to each other. For this dataset, there are 440 elements (n=440) and 6 measurements (m=6), so the covariance matrix would be a 440x6 matrix; with 6 eigenvalues. The total variance is the Trace of the S, which is just the summation of the main diagonal. Next the covariance matrix is decomposed into its eigenvectors and eigenvalues; the eigenvectors of S represents the principal components (directions of maximum variance and determined the direction of the new feature space) and the eigenvalues (scalar) correspond to the magnitude of the eigenvectors; in other words, the eignevalues explains the variance along the new featue axes. The eigenvalues are sorted in descending order (largest first) and dividing each eigenvalue by the total variance yields the fraction of variance explained. We are only interested in the top eigenvectors based on the values of their corresponding eigenvalues. The eigenvector corresponding to the highest eigenvalue will be the first PC. The eigenvector (v1) points in the direction of maximum variance (most information gained) of the dataset. The second eigenvector (v2) will point in direction orthogonal (perpendicular in 2D) to v1. One of the main advantages of using PCA is if you have a dataset with high dimensions you can reduce the number dimensions without losing much of the information. Using feature extraction, the data is projected onto a new feature space and thus reducing the number dimensions through data compression; the goal is to maintain the most relevant information. Being that the objective is to reduce the dimensionality of the data by compressing it onto a new feature subspace, we chose only the subset of the eigenvectors (principal components) that contains most of the information (variance). \n",
    "\n",
    "\n",
    "- The dimensions of the first PC (explains the most variability in the data) and has coefficients of `Fresh = -0.976`, `Frozen = -0.152`,`Milk = -0.121`, etc. We can clearly see that fresh items have the largest magnitude, followed by frozen products, and then milk. The descriptive statistics shows that on average, fresh items are purchased 55% more than the other 5 products. The annual spending on fresh items has the largest standard deviation (12,647) which suggest there is a lot of variance between the customers on how much they spend on fresh items. As the first PC identifies the item as the most important factor among the features, it would be a good idea to increase marketing towards the customer base on fresh products as it seems to be a cluster of those who buy large quantities  of fresh products while there are customers who make smaller purchase orders. \n",
    "\n",
    "\n",
    "- Second Principal Component gives the following magnitudes for the eigenvector v2: `Fresh = -0.110614` `Milk =0.515802`, `Grocery =0.764606`, `Frozen = -0.018723` etc. Looking at the magnitude of the eigenvector, a correlation exist among `Grocery` and `Milk`; customers who buy more 'grocery' items also tend to buy more 'milk' items as well. Therefore, it would be advised to market grocery and milk products together given their correlation in order to increase sales. Bundling these two items when shipping to the customer will also reduce cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply PCA with the same number of dimensions as variables in the dataset\n",
    "\n",
    "#Subtracting out the mean to center the data:\n",
    "columns = data.columns\n",
    "mean = data.mean()\n",
    "df_m = pd.DataFrame(mean)\n",
    "df_mean = df_m.transpose()\n",
    "data_center = pd.DataFrame(data[columns].values - df_mean[columns].values, columns=columns)\n",
    "#print data_center\n",
    "\n",
    "def doPCA():\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(data_center)\n",
    "    return pca\n",
    "\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "pca = doPCA()\n",
    "columns = data_center.columns\n",
    "df_ica = pd.DataFrame(pca.components_, columns = columns, index=['1', '2', '3', '4','5'])\n",
    "df_ica.index.names = ['PC']\n",
    "print \"Principal Component Analysis:\"\n",
    "print df_ica\n",
    "\n",
    "ex_var = pca.explained_variance_ratio_\n",
    "df_var = pd.Series(ex_var,index=['1','2','3','4','5'])\n",
    "df_var.sort(ascending=False)\n",
    "\n",
    "\n",
    "df_var.index.names = ['PC']\n",
    "print \"\\nExplained Variance of Each Component:\"\n",
    "print df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8,5))\n",
    "x = np.arange(1,6)\n",
    "y = np.cumsum(ex_var)\n",
    "pl.plot(x,y,marker =\"o\", mfc='#780000', color = '#CC0000')\n",
    "pl.xlabel(\"No. of PC Components\", fontsize = 14)\n",
    "pl.ylabel(\"Cumulative Explained Variance Ratio\", fontsize =14)\n",
    "pl.title(\"No. of PC Components vs Explained Variance Ratio\", fontsize = 16)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,6))\n",
    "pl.bar(range(1,6), ex_var, alpha = .8, align='center',\n",
    "       label = 'Individual Explained Variance', color = 'blue')\n",
    "pl.ylabel('Explained Variance Ratio', fontsize = 14)\n",
    "pl.xlabel('Princial Components', fontsize = 14)\n",
    "pl.title('Explained Variance', fontsize = 16)\n",
    "pl.show\n",
    "\n",
    "#Slope:\n",
    "X1, Y1 = 1, 0.459614\n",
    "X2, Y2 = 2, 0.405172\n",
    "X3, Y3 = 3, 0.070030\n",
    "X4, Y4 = 4, 0.044023\n",
    "slope1 = (Y2-Y1)/(X2-X1)\n",
    "slope2 = (Y3-Y2)/(X3-X2)\n",
    "slope3= (Y4-Y3)/(X4-X3)\n",
    "print \"Slope1:\", slope1\n",
    "print \"Slope2:\", slope2\n",
    "print \"Slope3:\", slope3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def biplot(df):\n",
    "    # Fit on 2 components\n",
    "    #Scaling \n",
    "    pca = PCA(n_components=2, whiten=True).fit(df)\n",
    "    \n",
    "    # Plot transformed/projected data\n",
    "    ax = pd.DataFrame(\n",
    "        pca.transform(df),\n",
    "        columns=['PC1', 'PC2']\n",
    "    ).plot(kind='scatter', x='PC1', y='PC2', figsize=(8, 6), color = 'g', s=5)\n",
    "    # Plot arrows and labels\n",
    "    for i, (pc1, pc2) in enumerate(zip(pca.components_[0], pca.components_[1])):\n",
    "        ax.arrow(0, 0, pc1, pc2, width=0.001, fc='r', ec='r')\n",
    "        ax.annotate(df.columns[i], (pc1, pc2), size=12)\n",
    "    return ax\n",
    "\n",
    "ax = biplot(data)\n",
    "pl.title(\"Biplot\", fontsize = 20)\n",
    "pl.xlabel(\"Principal Component: 1\", fontsize = 14)\n",
    "pl.ylabel(\"Principal Component: 2\", fontsize = 14)\n",
    "ax.set_xlim([-1.5, .5])\n",
    "ax.set_ylim([-1.0, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?\n",
    "\n",
    "- [`Independent Component Analysis`](http://scikit-learn.org/stable/modules/decomposition.html#ica) is an algorithm that finds directions in the feature space corresponding to projections with high non-Gaussianity. ICA will provide six new vectors with each vector being an independent component. Furthermore with ICA, we can attempt to identify the different types of customers so that we can understand more of their consumption patterns. Before running ICA, we can use a preprocessing technique such as [`Scale`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) to center the data to zero; removing the mean can help measure the variance of the data now from the origin (origin). This could still be done by keeping the mean, the only difference now is that the center is the mean itself, and not 0. The calculations are more simpler when you remove the mean as oppose to keeping it. Also, having the values centered at zero can capture the distributions when transforming the data.\n",
    "\n",
    "\n",
    "- ICA1: Mostly includes frozen items (highest magnitude); customers tend to buys more grocery/frozen items and less detergents_paper/delicatessen/fresh/milk products.\n",
    "\n",
    "\n",
    "- ICA2: Customer who buys more fresh products buys less milk/detergents_paper/delicatessen\n",
    "\n",
    "\n",
    "- ICA3: Most significant is orders for delicatessens while decrease in the remaining products\n",
    "\n",
    "\n",
    "- ICA4: Grocery has the highest magnitude (0.11); increase in grocery purchases is inversely related to detergents (decrease).\n",
    "\n",
    "\n",
    "- ICA5: Customer purchases mostly milk/grocery/ will buy less slightly less fresh items and detergents (perhaps small market store)\n",
    "\n",
    "\n",
    "- ICA6: Primarly as purchase orders for grocery increases, milk/fresh decrease  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adjust the data to have center at the origin:\n",
    "columns = data.columns\n",
    "scale = preprocessing.scale(data)\n",
    "df_scale = pd.DataFrame(scale, columns=columns)\n",
    "#print data_scaled.mean()\n",
    "#print data_scaled.std()\n",
    "\n",
    "def doICA():\n",
    "    ica = FastICA(n_components = 6, random_state=42)\n",
    "    ica.fit(df_scale)\n",
    "    return ica\n",
    "\n",
    "# Print the independent components\n",
    "ica = doICA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ica = pd.DataFrame(ica.components_,columns=columns, index=['1','2','3','4','5','6'])\n",
    "df_ica.index.names = ['IC']\n",
    "print \"\\nUnmixing Matrix:\"\n",
    "df_ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(20,20))\n",
    "pl.figure(figsize = (11,5))\n",
    "pl.title('Independent Component Analysis', fontsize=18)\n",
    "sns.heatmap(df_ica, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then sample elements from the clusters to understand their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?\n",
    "\n",
    "- In unsupervised learning, we are not predicting an outcome (no target label is given \"y\"); we are interested in how the features are related. We can group the data into \"clusters\" and two common techniques are K-means and Gaussian Mixture Models.\n",
    "\n",
    "\n",
    "- K-mean is an iterative algorithm that is based on the idea that there exist a specific number of clusters or groups in the dataset and finds clusters that consists of similar characteristics that are more related to each other than in other groups. Each group in the data is distributed around a central point called the \"centroid\" which is the average of the cluster. K-means begins by initially guessing the K-number of clusters and then the algorithm randomly picks the centroid as the initial clusters of the data (not the correct centers for the first iteration). All of the points are assigned to the nearest centroid based on the Euclidian distance and is grouped into a cluster. The new centroid will be the mean of data points assigned to the corresponding cluster. This process is repeated until either the number of specified iterations is reached or until the clustering assignments do not change. Using K-Means in scikit-learn, the iteration will stop early if it converges before the maximum number of iterations is reached. As a note, incorrectly assigning the number of clusters for k can result in poor clustering performance. Next, each sample is assigned to the nearest centroid and then move the centroid to the center of the corresponding samples. \n",
    "\n",
    "\n",
    "- K- mean is a hard clustering algorithm, where the clusters to not overlap (i.e. the data point is either \"blue\" or \"green\". The advantages of using K-means is due to it's simplicity, speed, and scalability of larger numbers of data points. However, one weakness when using this method is that the initial centroids are placed randomly, which could result in a bad starting spot and the iteration could stop at an unlikely solution. Thus numerous iterations need to be run which can come at a cost of more time required to run K-means, however the trade-off is that more iteration that are conducted, the better the results will be.  \n",
    "\n",
    "\n",
    "- Mixture models are a probabilistically way of doing soft clustering, where clusters may overlap. Gaussian Mixture Models (GMM) assumes that all of the data points in each cluster are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. GMM use a Gaussian distributions to find the most probable cluster that a point would belong to. The mixture model attempts to find unknown parameters such as the mean/covariance of the Gaussian distribution. \n",
    "\n",
    "- The different clustering algorithms and their usage can be found [`here`](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Below is the code to visualize the clusters (Gaussian Mixture Model & K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Reduce to Two dimensions using PCA to capture variation\n",
    "reduced_data = PCA(n_components = 2).fit_transform(data)\n",
    "df_rd = pd.DataFrame(reduced_data)\n",
    "df_rd[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to evaluate the quality of the clustering, we need to find the optimal number of clusters. This can be done by computing the inertia, a method for comparing the performance of different k-means clusters (n_clusters). Inertia is calculated by taking the summation of the difference between every data point in each cluster and it's respected centroid. Inertia is accessible is calling the `inertia__` function, which computes with within cluster SSE. If the data points in the cluster are similar to it's centroid, the difference is small, resulting in a small inertia. Plotting the number of clusters vs the inertia yield a plot below. What is important from this plot is to observe the rate of change. We see that as the number of clusters increase, the lower the inertia will be. This is due to the samples being closer to their corresponding centroids. Basically, the idea behind this plot is to identify the value of k where the inertia begins to increase most rapidly, thus the optimal number of clusters to consider in this model is between 4-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_clusters():\n",
    "    inertia = []\n",
    "    delta_inertia = []\n",
    "    for i in range(1,11):\n",
    "        clustering = KMeans(n_clusters = i,\n",
    "                    n_init=10,\n",
    "                    max_iter=300,\n",
    "                    random_state=1)\n",
    "        clustering.fit(reduced_data)\n",
    "        if inertia: \n",
    "            delta_inertia.append(inertia[-1] - clustering.inertia_)\n",
    "        inertia.append(clustering.inertia_)\n",
    "    \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.plot([k for k in range(1,10)], delta_inertia, marker='o', color='g')\n",
    "    pl.title('Optimal Number of Clusters', fontsize=18)\n",
    "    pl.xlabel('Number of Clusters', fontsize=14)\n",
    "    pl.ylabel('Rate of Change (Inertia)', fontsize=14)\n",
    "    pl.show\n",
    "    \n",
    "number_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cluster_plot_gmm():   \n",
    "    n = 5\n",
    "    # TODO: Implement clustering algorithm and fitting to the reduced data for visualization\n",
    "    gmm = GMM(n_components=n, n_iter=300)\n",
    "    clusters = gmm.fit(reduced_data)\n",
    "    centroids = clusters.means_\n",
    "    \n",
    "    x_gmm = clusters.predict(reduced_data)\n",
    "\n",
    "    # Which customers are in which cluster: K-Means\n",
    "    for i in range(0,5):\n",
    "        count = data[x_gmm==i].shape[0]\n",
    "        print 'Gaussian Mixture Model:'\n",
    "        print 'Cluster Number: %s' % (i+1)\n",
    "        print 'Number of samples in cluster: %s' % count\n",
    "        print data[x_gmm==i][list(data.columns[:-1])].mean()\n",
    "\n",
    "    # Plot the decision boundary by building a mesh grid to populate a graph.\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    hx = (x_max-x_min)/1000.\n",
    "    hy = (y_max-y_min)/1000.\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    pl.figure(figsize=(8,6))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure(1)\n",
    "    pl.clf()\n",
    "    pl.imshow(Z, interpolation='nearest',\n",
    "                extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                cmap=pl.cm.Paired,\n",
    "                aspect='auto', origin='lower')\n",
    "\n",
    "    pl.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=5)\n",
    "    pl.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=150, linewidths=4,\n",
    "                color='w', zorder=10)\n",
    "    pl.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "            'White X=Centroids; No. of Clusters:%s'%(5), fontsize = 14)\n",
    "    pl.xlim(x_min, x_max)\n",
    "    pl.ylim(y_min, y_max)\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "    # Annotate Plot:\n",
    "    labels = ['{0}'.format(i) for i in range(1,6)]\n",
    "    for label, x, y in zip(labels, centroids[:,0], centroids[:,1]):\n",
    "         pl.annotate(label, xy = (x, y), xytext = (-20, 20),\n",
    "         textcoords = 'offset points', size=14, ha = 'right', va = 'bottom',\n",
    "         bbox = dict(boxstyle=\"round4,pad=0.3\", fc=\"cyan\", ec=\"b\", lw=2),\n",
    "         arrowprops = dict(arrowstyle = 'simple', connectionstyle = 'arc3,rad=-0.3'))\n",
    "    pl.show()\n",
    "    \n",
    "    data['cluster'] = clusters.predict(reduced_data)\n",
    "    results = pd.DataFrame(data=labels, columns=['cluster']) \n",
    "    data_grouped = data.groupby('cluster').mean()\n",
    "    \n",
    "    # Histogram\n",
    "    ax = data_grouped.plot(kind='bar',legend=True, \n",
    "                    figsize=(10,6), alpha=.5, title=\"Customer Breakdown\")\n",
    "    ax.set_xlabel(\"Clusters\",fontsize=16)\n",
    "    ax.set_ylabel(\"Total Monetary Units\",fontsize=16)\n",
    "    ax.set_title(ax.get_title(), fontsize=20)\n",
    "    pl.show()\n",
    "\n",
    "cluster_plot_gmm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cluster_plot_kmeans():  \n",
    "    num_clusters = 5\n",
    "    clusters = KMeans(n_clusters=num_clusters, n_init=100)\n",
    "    clusters.fit(reduced_data)\n",
    "    centroids = clusters.cluster_centers_\n",
    "    \n",
    "    x_km = clusters.predict(reduced_data)\n",
    "\n",
    "    # Which customers are in which cluster: K-Means\n",
    "    for i in range(0,5):\n",
    "        count = data[x_km==i].shape[0]\n",
    "        print 'Kmeans:'\n",
    "        print 'Cluster Number: %s' % (i+1)\n",
    "        print 'Number of samples in cluster: %s' % count\n",
    "        print data[x_km==i][list(data.columns[:-1])].mean()\n",
    "\n",
    "    # Plot the decision boundary by building a mesh grid to populate a graph.\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    hx = (x_max-x_min)/1000.\n",
    "    hy = (y_max-y_min)/1000.\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    pl.figure(figsize=(8,6))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure(1)\n",
    "    pl.clf()\n",
    "    pl.imshow(Z, interpolation='nearest',\n",
    "                extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                cmap=pl.cm.Paired,\n",
    "                aspect='auto', origin='lower')\n",
    "\n",
    "    pl.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=5)\n",
    "    pl.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=150, linewidths=4,\n",
    "                color='w', zorder=10)\n",
    "    pl.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "            'White X=Centroids; No. of Clusters:%s'%(5), fontsize = 14)\n",
    "    pl.xlim(x_min, x_max)\n",
    "    pl.ylim(y_min, y_max)\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "    # Annotate Plot:\n",
    "    labels = ['{0}'.format(i) for i in range(1,6)]\n",
    "    for label, x, y in zip(labels, centroids[:,0], centroids[:,1]):\n",
    "         pl.annotate(label, xy = (x, y), xytext = (-20, 20),\n",
    "         textcoords = 'offset points', size=14, ha = 'right', va = 'bottom',\n",
    "         bbox = dict(boxstyle=\"round4,pad=0.3\", fc=\"cyan\", ec=\"b\", lw=2),\n",
    "         arrowprops = dict(arrowstyle = 'simple', connectionstyle = 'arc3,rad=-0.3'))\n",
    "    pl.show()\n",
    "    \n",
    "    print \"K-Means:\"\n",
    "    print \"Number of Customers per Cluster:\"\n",
    "    print data['cluster'].value_counts()\n",
    "\n",
    "    data['cluster'] = clusters.labels_\n",
    "    results = pd.DataFrame(data=labels, columns=['cluster']) \n",
    "    data_grouped = data.groupby('cluster').mean()\n",
    "    \n",
    "    # Histogram\n",
    "    ax = data_grouped.plot(kind='bar',legend=True, \n",
    "                    figsize=(10,6), alpha=.5, title=\"Customer Breakdown\")\n",
    "    ax.set_xlabel(\"Clusters\",fontsize=16)\n",
    "    ax.set_ylabel(\"Total Monetary Units\",fontsize=16)\n",
    "    ax.set_title(ax.get_title(), fontsize=20)\n",
    "    pl.show()\n",
    "\n",
    "cluster_plot_kmeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7**) What are the central objects in each cluster? Describe them as customers.\n",
    "\n",
    "- The central object of each cluster is in reference to the above Gaussian Mixture Model (GMM) and K-Means cluster plot and histogram. For the sake of this discussion, we will explore the results from the GMM. As a note on the histogram, the cluster labeling begins at 0; this refers to cluster 1 and so on. What we find in the first cluster is a higher precentage of fresh, milk, grocery, and frozen products but with lowest consumption among the remaining clusters. To the left of cluster 1 is the second cluster (green) which contains 133 samples. The fresh items are the most significant, which reflects the results discussed earlier with `fresh` having the highest magnitude in the first PC, thus more influential in the results. The first two clusters indiciate this customer base to be more smaller familiy run corner store or convience store with lower consumption patterns. The third cluster shows larger consumption patterns which would classify this group as the larger grocery stores such as Whole Foods or Trader Joes. Only 17 stores fall into this cluster, which is just a small fraciton of the total customer base. The fourth cluster has a total of 38 customers and the most significant item in this cluster is a high consumption of fresh products. Consumption for Milk, grocery, and frozen items drops off by nearly 75%. Perhaps this cluster represents the patterns associated with a restaurant that has a high density of fresh products such as a deli. The fifth cluster consists of a customer base that would most likely represent a smaller neighborhood grocery store with the bulk of the products purchased being fresh/milk/grocery items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**8**) Which of these techniques did you feel gave you the most insight into the data?\n",
    "\n",
    "- Applying Principal Component Analysis was one of the most useful techniques as it provides insight into the data. We were able to reduce the high dimensional dataset to a lower dimensional space which allowed the data to be visualized on a two dimensional scatter plot. The data was projected onto the new principal component axes and each datapoint in the plot represents a single customer (store). Additionally, PCA is a feature extractor, thus we are able to identify which attributes are most significant in each principal component. After applying PCA, the second most useful method is then implementing GMM to the reduced dataset in order to group the customers into respective market segments, which can enhance better decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9**) How would you use that technique to help the company design new experiments?\n",
    "\n",
    "- As we have classified the wholesale customers into different markets, applying the A/B test would help the company with making any changes to the business model. A/B testing, often called split testing, is widely used when creating web pages by comparing two versions of a web page and see which one performs better. Thus, applying A/B testing to this dataset will allow the company to make optimal decisions by trying out possible changes and seeing what performs better. For an example, we may want to test various pricing options for products to monitor how the change in price affects purchase rate for a particular market. We can split the cluster into two groups with the goal of determining if the rate of success is higher in one group (A) than the other (B). One way to do this is determine the p-value to get an idea of how confident we are that the two groups have different chances of success. Using the common baseline of \"statistically significant\" at 0.05, a p-value lower than 0.05 means the two groups are different. But the estimated improvement in success rate would give a better idea of how much better/worse any changes might be compared to initial prices. There are numerous instances on what could be analyzed, but this example would just give an idea on how we can make optimal business decisions by applying any new changes to how the company operates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10**) How would you use that data to help you predict future customer needs?\n",
    "\n",
    "- Once we have defined the customer base, we can then target particular customers by performing different tests in order to enhance profit and growth of the company. After applying PCA and GMM, we have grouped the data into clusters and now the company can utilize a supervised machine learning model, either regression or classification, to predict which cluster the new customer is likely to fall in. For instance, we could extract the customers and their corresponding consumption elements (Fresh, frozen, etc.) from the Gaussian Mixture Model and use then use their respected cluster as the label to use the classification learning algorithm. This would then allow the company to make prediction on future customers and their purchase orders. Another suggestion would be to use regression on the clusters that were identified in order to predict the current wholesale customers in addition to any new customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
